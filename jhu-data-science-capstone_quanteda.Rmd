---
title: "Coursera Data Science Capstone - Prediction model prototype with Quanteda"
author: "Samy Soualem"
date: "September 14, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document is just a technical memo for personal use.

```{r echo = FALSE, results = FALSE, message = FALSE}
library(quanteda)
library(ggplot2)
library(doParallel)
library(data.table)

# PARAMETERS : modify if necessary
# If USE_SAMPLE_DATA is FALSE, all the data will be used
# If USE_SAMPLE_DATA is TRUE, only a random subset of FILE_SAMPLE_RATE will be used. 
USE_SAMPLE_DATA <- TRUE
# sample rate for the original files. This subset will be used to create the training / validation / test sets
FILE_SAMPLE_RATE <- 0.3
TRAINING_SAMPLE_RATE <- 0.6 # Rate for validation and test sets are the same
MAX_NB_PREDICT <- 5 # maximum number of predictions
MAX_N_INPUT_NGRAM <- 3  # maximum number of words for an input ngram
LAMBDA_SBO <- 0.4 # lambda parameter for stupid backoff
NB_CORES <- 4
TOP_NGRAM_LIMIT <- 50 # Number of ngrams to plot
# minimum number of occurences a ngram needs to have to be kept in the DFM
# For reference, the Google cut-off point was at least 40 occurences for 1,024,908,267,229 in total
MIN_NGRAM_COUNT <- 2
RDS_FILE_VERSION <- "v0.13"  # to know from which model a RSD file was created

registerDoParallel(cores=NB_CORES)
```

```{r echo = FALSE, results = FALSE}
# Data source : https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip
# Data directories
enUsDir <- "data/final/en_US"
enUsSampleDir <- file.path(enUsDir, paste0("sample_", FILE_SAMPLE_RATE))
enFileNames <- c("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt")
enFile <- file.path(enUsDir, enFileNames)


fileSetsSuffix <- c("train.txt", "validation.txt", "test.txt")
blogSampleFile <- file.path(enUsSampleDir
                  , paste("en_US.blogs_sample", FILE_SAMPLE_RATE,   
                      fileSetsSuffix , sep = "_"
                    )
                  )
newsSampleFile <- file.path(enUsSampleDir
                  , paste("en_US.news_sample", FILE_SAMPLE_RATE,   
                      fileSetsSuffix , sep = "_"
                    )
                  )
twitterSampleFile <- file.path(enUsSampleDir
                  , paste("en_US.twitter_sample", FILE_SAMPLE_RATE,   
                      fileSetsSuffix , sep = "_"
                    )
                  )
                           
enSampleFile <- list(blogSampleFile, newsSampleFile, twitterSampleFile)

swearWordsFile <- "data/swearWords.csv"
rdsDir <- paste("data/rds", RDS_FILE_VERSION, paste("sample", FILE_SAMPLE_RATE, sep = '_') , sep = "/")
```

## RDS files preparation to save the results
```{r}
if (! dir.exists(rdsDir)) {
  dir.create(rdsDir, recursive = TRUE)
}

rdsSampleRate <- paste("sample", FILE_SAMPLE_RATE, sep ='_')
rdsSuffix <- ".rds"

# Version and sample rate in file name just in case
rdsFileSentences <- file.path(rdsDir, paste0(paste(RDS_FILE_VERSION, rdsSampleRate, "sentenceTokens", sep = '_'), rdsSuffix))
rdsFileDtDfm1 <- file.path(rdsDir, paste0(paste(RDS_FILE_VERSION, rdsSampleRate, "dtDfm1", sep = '_'), rdsSuffix))
rdsFileDtDfm2 <- file.path(rdsDir, paste0(paste(RDS_FILE_VERSION, rdsSampleRate, "dtDfm2", sep = '_'), rdsSuffix))
rdsFileDtDfm3 <- file.path(rdsDir, paste0(paste(RDS_FILE_VERSION, rdsSampleRate, "dtDfm3", sep = '_'), rdsSuffix))
rdsFileDtDfm4 <- file.path(rdsDir, paste0(paste(RDS_FILE_VERSION, rdsSampleRate, "dtDfm4", sep = '_'), rdsSuffix))
```

## Creating training, validation and test sets files

Total file sample rate : `r I(FILE_SAMPLE_RATE * 100)`%.

Training set ratio : `r I(TRAINING_SAMPLE_RATE * 100)`%

Validation set ratio : `r I((1 - TRAINING_SAMPLE_RATE) / 2 * 100)`%

Test set ratio : `r I((1 - TRAINING_SAMPLE_RATE) / 2 * 100)`%

```{r echo = FALSE, results = FALSE}
# Stop with an error if the original files are not found
if (! all(file.exists(enFile))) {
  errMsg <- paste("The following files were not found :", paste(enFile, collapse = ", "))
  stop(errMsg)
}


if (USE_SAMPLE_DATA) {
  # If all the sample files don't already exist for this sample rate, create them
  if (! all(file.exists(unlist(enSampleFile)))) {
    if (! dir.exists(enUsSampleDir)) {
      dir.create(enUsSampleDir)
    }
    
    # Read all files, generate a random sample and save it to another file for later use
    set.seed(1234)
    for(i in 1:length(enFile)) {
      tempData <- readLines(enFile[i])
      # Only keep a random subset before creating the training, validation and test sets
      # Done because files are large and we want the validation and test sets to have held out samples
      tempData <- sample(tempData, as.integer(FILE_SAMPLE_RATE * length(tempData)))
      
      # Training set
      inTraining <- seq_along(tempData)
      inTraining <- sample(inTraining, as.integer(TRAINING_SAMPLE_RATE * length(inTraining)))
      trainSet <- tempData[inTraining]
      
      # Validation and test sets
      validAndTestSet <- tempData[-inTraining]
      inTest <- seq_along(validAndTestSet)
      inTest <- sample(inTest, as.integer(0.5 * length(inTest)))  # 0.5 b/c validation and test sets have same size
      testSet <- validAndTestSet[inTest]
      validSet <- validAndTestSet[-inTest]
      
      # Save sampled datasets in files
      writeLines(trainSet, enSampleFile[[i]][1])
      writeLines(validSet, enSampleFile[[i]][2])
      writeLines(testSet, enSampleFile[[i]][3])
    }
    # Free memory (not very useful in general apparently)
    rm(tempData)
    rm(trainSet)
    rm(validSet)
    rm(testSet)
    gc()  # garbage collector
  }
  # Sample files here, nothing to do
}
```


## Load training set into a Corpus
```{r cache = TRUE, echo = FALSE, results = FALSE}
# Only load Corpus if the tokenized sentences don't exist in a RDS file for this version and sample rate
if(!file.exists(rdsFileSentences)) {
  if (USE_SAMPLE_DATA) {
    corpusFiles <- character()
    
    # For each document, only load the training set
    for(i in 1:length(enSampleFile)) {
      corpusFiles <- c(corpusFiles, enSampleFile[[i]][1]) # Training set always first
    }
  } else {
    corpusFiles <- enFile
  }
  # Load documents as a Quanteda Corpus
  enRawCorpus <- corpus(textfile(corpusFiles, encoding = "UTF-8"))
  #summary(enRawCorpus)
}
```

## Tokenizing functions
The corpus should be tokenized as sentences before creating the n-grams tokens. This makes sure that no n-gram will be created with the end of sentence and the beginning of the next.
```{r results = FALSE}
# Get vector of swear words to remove
# File downloaded from http://www.bannedwordlist.com/lists/swearWords.csv
getSwearWords <- function() {
  swearWords <- readLines(swearWordsFile, skipNul = TRUE, warn = FALSE) # warning because of missing "end of line" character
  swearWords <- unlist(strsplit(swearWords, ",", fixed = TRUE))
  swearWords
}

# Sentence tokenization function
tokenizeSentences <- function(x) {
  # TODO : remove twitter characters here manually
  
  tokens <- tokenize(x, what = "sentence", removeNumbers = TRUE, removePunct = TRUE, removeSymbols = TRUE
                     , removeSeparators = TRUE, removeTwitter = FALSE, removeHyphen = TRUE, removeURL = TRUE)
  
  # Add start and end of sentences symbols
  # #bos# = beginning of sentence
  # #eos# = end of sentence (like EOF symbol for "end of file")
  
  # Symbols added because of the Ngrams chapter in the book Speech and Language Processing
  # by Daniel Jurafsky and James Martin
  # Not sure if these symbols will be used in the prediction model
  # WARNING : these symbols cannot use punctuation that would be removed in the n-grams tokenization
  
  # ************************************************************************************
  # TODO : see if start and end of sentences useful for final model.
  # => If used : 
  #       - set removeTwitter to FALSE in the tokenizeNgrams function
  #       - Remove Twitter content with regexp manually after the ngram tokenization
  # ************************************************************************************
  #unlist(lapply(tokens, function(y) paste('#bos#', toLower(y), '#eos#')))
  unlist(lapply(tokens, toLower))
}

# N-grams tokenization function
# Default is unigram if n is not set
# removeTwitter needs to be FALSE in case start and end of sentence markers used (# character)
tokenizeNgrams <- function(x, n = 1L) {
  tokenize(x, what = "word", removeNumbers = TRUE, removePunct = TRUE, removeSymbols = TRUE
           , removeSeparators = TRUE, removeTwitter = TRUE, removeHyphen = TRUE, removeURL = TRUE
           , ngrams = n, simplify = TRUE)
}
```

## Tokenizing
### Sentences
```{r cache = TRUE}
# If a previous RDS file exists for this version and sample rate, load it
# Else compute the tokens and save it to a RDS file
if(file.exists(rdsFileSentences)) {
  sentenceTokens <- readRDS(rdsFileSentences)
} else {
  # Always tokenize sentences first
  #sentenceTokens <- tokenizeSentences(enRawCorpus)
  system.time(sentenceTokens <- tokenizeSentences(enRawCorpus))
  saveRDS(sentenceTokens, rdsFileSentences)
  
  # Free memory (not very useful in general apparently)
  rm(enRawCorpus)
  gc()
}
```

### 1-grams
Features are ignored in the dfm step to avoid creating wrong ngrams when n > 1 (because of a word being removed from a ngram).

See the "Details" section of dfm's help page.
```{r cache = TRUE}
# If a previous RDS file exists for this version and sample rate, load it
# Else compute the tokens and save it to a RDS file
if(file.exists(rdsFileDtDfm1)) {
  dtDfm1 <- readRDS(rdsFileDtDfm1)
} else {
  #ngram1 <- tokenizeNgrams(sentenceTokens, 1)
  system.time(ngram1 <- tokenizeNgrams(sentenceTokens, 1))
  #dfm1 <- dfm(ngram1, ignoredFeatures=getSwearWords())
  system.time(dfm1 <- dfm(ngram1, ignoredFeatures=getSwearWords()))
  # 1.1 Gb for 15%
  
  # Convert DFM to data table format (faster and uses less memory)
  dtDfm1<- data.table(ngram = features(dfm1), count = as.integer(colSums(dfm1)), key = "ngram")
  saveRDS(dtDfm1, rdsFileDtDfm1)
  
  # Free memory (not very useful in general apparently)
  rm(ngram1)
  rm(dfm1)
  gc()
}
```

### 2-grams
```{r cache = TRUE}
# If a previous RDS file exists for this version and sample rate, load it
# Else compute the tokens and save it to a RDS file
if(file.exists(rdsFileDtDfm2)) {
  dtDfm2 <- readRDS(rdsFileDtDfm2)
} else {
  #ngram2 <- tokenizeNgrams(sentenceTokens, 2)
  system.time(ngram2 <- tokenizeNgrams(sentenceTokens, 2))
  #dfm2 <- dfm(ngram2, ignoredFeatures=getSwearWords())
  system.time(dfm2 <- dfm(ngram2, ignoredFeatures=getSwearWords()))
  
  # Convert DFM to data table format (faster and uses less memory)
  dtDfm2<- data.table(ngram = features(dfm2), count = as.integer(colSums(dfm2)), key = "ngram")
  saveRDS(dtDfm2, rdsFileDtDfm2)
  
  # Free memory (not very useful in general apparently)
  rm(ngram2)
  rm(dfm2)
  gc()
}
```

### 3-grams
```{r cache = TRUE}
# If a previous RDS file exists for this version and sample rate, load it
# Else compute the tokens and save it to a RDS file
if(file.exists(rdsFileDtDfm3)) {
  dtDfm3 <- readRDS(rdsFileDtDfm3)
} else {
  #ngram3 <- tokenizeNgrams(sentenceTokens, 3)
  system.time(ngram3 <- tokenizeNgrams(sentenceTokens, 3))
  #dfm3 <- dfm(ngram3, ignoredFeatures=getSwearWords())
  system.time(dfm3 <- dfm(ngram3, ignoredFeatures=getSwearWords()))
  # 1.5 Gb for 15%
  
  # Convert DFM to data table format (faster and uses less memory)
  dtDfm3<- data.table(ngram = features(dfm3), count = as.integer(colSums(dfm3)), key = "ngram")
  saveRDS(dtDfm3, rdsFileDtDfm3)
  
  # Free memory (not very useful in general apparently)
  rm(ngram3)
  rm(dfm3)
  gc()
}
```

### 4-grams
```{r cache = TRUE}
# If a previous RDS file exists for this version and sample rate, load it
# Else compute the tokens and save it to a RDS file
if(file.exists(rdsFileDtDfm4)) {
  dtDfm4 <- readRDS(rdsFileDtDfm4)
} else {
  #ngram4 <- tokenizeNgrams(sentenceTokens, 4)
  system.time(ngram4 <- tokenizeNgrams(sentenceTokens, 4))
  #dfm4 <- dfm(ngram4, ignoredFeatures=getSwearWords())
  system.time(dfm4 <- dfm(ngram4, ignoredFeatures=getSwearWords()))
  # 1.7 Gb for 15%
  
  # Convert DFM to data table format (faster and uses less memory)
  dtDfm4<- data.table(ngram = features(dfm4), count = as.integer(colSums(dfm4)), key = "ngram")
  saveRDS(dtDfm4, rdsFileDtDfm4)
  
  # Free memory (not very useful in general apparently)
  rm(ngram4)
  rm(dfm4)
  gc()
}
```

```{r}
# Add all DFM data tables to one list
lDfm <- list(ngram1=dtDfm1, ngram2=dtDfm2, ngram3=dtDfm3, ngram4=dtDfm4)
```


## Analysing the most common n-grams
### Keep only the `r I(TOP_NGRAM_LIMIT)` most common n-grams for each document-term matrix
```{r}
dtFreq1Top <- head(dtDfm1[order(count, decreasing = TRUE)], TOP_NGRAM_LIMIT)
dtFreq2Top <- head(dtDfm2[order(count, decreasing = TRUE)], TOP_NGRAM_LIMIT)
dtFreq3Top <- head(dtDfm3[order(count, decreasing = TRUE)], TOP_NGRAM_LIMIT)
dtFreq4Top <- head(dtDfm4[order(count, decreasing = TRUE)], TOP_NGRAM_LIMIT)
```

### Plot the most common n-grams
```{r}
# Most common 1-grams plot
g1 <- ggplot(dtFreq1Top, aes(x = reorder(ngram, -count), y = count))
g1 <- g1 + geom_bar(stat="identity", fill = "deepskyblue4")
g1 <- g1 + ggtitle("Top unigrams sorted by frequency")
g1 <- g1 + xlab("Unigram") + ylab("Count")
g1 <- g1 + theme(axis.text.x=element_text(angle=45, hjust=1))
g1

# Most common 2-grams plot
g2 <- ggplot(dtFreq2Top, aes(x = reorder(ngram, -count), y = count))
g2 <- g2 + geom_bar(stat="identity", fill = "deepskyblue4")
g2 <- g2 + ggtitle("Top bigrams sorted by frequency")
g2 <- g2 + xlab("Bigram") + ylab("Count")
g2 <- g2 + theme(axis.text.x=element_text(angle=45, hjust=1))
g2

# Most common 3-grams plot
g3 <- ggplot(dtFreq3Top, aes(x = reorder(ngram, -count), y = count))
g3 <- g3 + geom_bar(stat="identity", fill = "deepskyblue4")
g3 <- g3 + ggtitle("Top trigrams sorted by frequency")
g3 <- g3 + xlab("Trigram") + ylab("Count")
g3 <- g3 + theme(axis.text.x=element_text(angle=45, hjust=1))
g3

# Most common 4-grams plot
g4 <- ggplot(dtFreq4Top, aes(x = reorder(ngram, -count), y = count))
g4 <- g4 + geom_bar(stat="identity", fill = "deepskyblue4")
g4 <- g4 + ggtitle("Top fourgrams sorted by frequency")
g4 <- g4 + xlab("Fourgram") + ylab("Count")
g4 <- g4 + theme(axis.text.x=element_text(angle=45, hjust=1))
g4
```

## TODO : Prediction prototype
### Get n-gram containing the next-word candidates
```{r}
# DESCRIPTION
#   Returns n-grams containing the next-word candidates for the input n-gram
# INPUT
#   - inputNgram : n-gram with the format "W1_W2_W3" where WX are words. 1 to 3 words are acceptable
#   - ngramModel : list of 4 elements containing data.tables of the DFM for the model.
#       The data.tables in the list have 2 columns : ngram and count
#       The first element of the list is the 1-gram DFM, the second is the 2-grams DFM etc.#       
# OUTPUT
#   - data.table(ngram, count) :
#       Contains at most MAX_N_INPUT_NGRAM n-grams with their count number
#       If no match is found the output is empty
#       
getNgramMatch <- function(inputNgram, ngramModel) {
  match <- data.table(ngram = character(0), count = integer(0))
  
  # Input n-gram level
  n <- length(unlist(strsplit(inputNgram, "_", fixed = TRUE)))
  
  if(n > MAX_N_INPUT_NGRAM) {
    errorMsg <- paste("The input n-gram", paste0("\"", inputNgram, "\""), "has more than", MAX_N_INPUT_NGRAM, "words.")
    stop(errorMsg)
  }
 
  regex <- paste0("^", inputNgram, "_")
  # n+1 because if input is a trigram, lookup needs to be done in the fourgram model
  # Return at most MAX_NB_PREDICT of the most frequent matches
  match <- head(ngramModel[[n+1]][ngram %like% regex, ][order(count, decreasing = TRUE)], MAX_NB_PREDICT)
  
  # Less than MAX_NB_PREDICT matches found : try to match a lower-level ngram
  if(nrow(match) < MAX_NB_PREDICT) {
    # Remove first word and try to match this new ngram
    if(n >=2) {
      croppedNgram <- unlist(strsplit(inputNgram, "_", fixed = TRUE))
      croppedNgram <- croppedNgram[2:length(croppedNgram)]
      croppedNgram <- paste(croppedNgram, collapse = "_")
      # Add lower-level ngrams to the current level ngrams
      match <- rbind(match, getNgramMatch(croppedNgram, ngramModel))
    # If not enough matches to reach MAX_NB_PREDICT : return most common unigrams to complete the set
    } else {
      # Add lower-level ngrams to the current level ngrams
      match <- rbind(match, head(ngramModel[[1]][order(count, decreasing = TRUE)], MAX_NB_PREDICT))
    }
  }
  match
}

# TODO : unit test (for perf bench too)
# TU : check that recursivity works of only 1 match per level

# Tests from Quiz 2
getNgramMatch("a_case_of", lDfm)
getNgramMatch("would_mean_the", lDfm)
getNgramMatch("make_me_the", lDfm)
getNgramMatch("struggling_but_the", lDfm)
getNgramMatch("date_at_the", lDfm)
getNgramMatch("be_on_my", lDfm)
getNgramMatch("in_quite_some", lDfm)
getNgramMatch("with_his_little", lDfm)
getNgramMatch("faith_during_the", lDfm)
getNgramMatch("you_must_be", lDfm)



# TODO : define other unit tests
getNgramMatch("improbable_foobar_be", lDfm) # only 1-gram matches
getNgramMatch("improbable_foobar_toto", lDfm) # no match : only most common unigrams returned

# TODO : backoff / smoothing (=??) / score calculation
```

### Scoring function (stupid backoff)
```{r}
# DESCRIPTION
#   Returns the stupid backoff score of the input n-gram
# INPUT
#   - inputNgram : n-gram with the format "W1_W2_W3_W4" where WX are words. 1 to 4 words are acceptable
#   - ngramModel : list of 4 elements containing data.tables of the DFM for the model.
#       The data.tables in the list have 2 columns : ngram and count
#       The first element of the list is the 1-gram DFM, the second is the 2-grams DFM etc.#       
# OUTPUT
#   - Stupid backoff score for inputNgram (numeric)
#  
getSboScore <- function(inputNgram, ngramModel) {
  score <- 0
  
  # TODO : A LOT of perf optim to do because counts sometimes recalculated for nothing
  
  # Input n-gram level
  n <- length(unlist(strsplit(inputNgram, "_", fixed = TRUE)))
  
  if(n <= 0) {
    score <- 0
  } else {
      inputNgramCount <- ngramModel[[n]][inputNgram]$count
      
      if(!is.na(inputNgramCount) && inputNgramCount > 0) {
        if(n == 1) {
          lowerNgramCount <- nrow(ngramModel[[n]])
        # n >= 2
        } else {
          # Remove LAST word to get the lower level ngram
          lowerNgram <- unlist(strsplit(inputNgram, "_", fixed = TRUE))
          lowerNgram <- lowerNgram[1:length(lowerNgram) - 1]
          lowerNgram <- paste(lowerNgram, collapse = "_")
          
          lowerNgramCount <- ngramModel[[n - 1]][lowerNgram]$count
        }
        score <- inputNgramCount / lowerNgramCount
      } else {
        if(n >= 2) {
          # Remove FIRST word to get the lower level ngram
          lowerNgram <- unlist(strsplit(inputNgram, "_", fixed = TRUE))
          lowerNgram <- lowerNgram[2:length(lowerNgram)]
          lowerNgram <- paste(lowerNgram, collapse = "_")
          score <- LAMBDA_SBO * getSboScore(lowerNgram, ngramModel)
        }
      } 
  }
  score
}

getSboScore("a_case_of", lDfm)
getSboScore("would_mean_the", lDfm)
getSboScore("make_me_the", lDfm)
getSboScore("struggling_but_the", lDfm)
getSboScore("date_at_the", lDfm)
getSboScore("be_on_my", lDfm)
getSboScore("in_quite_some", lDfm)
getSboScore("with_his_little", lDfm)
getSboScore("faith_during_the", lDfm)
getSboScore("you_must_be", lDfm)

# TODO : define other unit tests
# Example 1 : 
#   check that getSboScore("struggling_but_the", lDfm) = LAMBDA_SBO * getSboScore("but_the", lDfm) if 3-gram not found
#   Do for all n-grams

getSboScore("improbable_foobar_be", lDfm) # only 1-gram matches
getSboScore("improbable_foobar_toto", lDfm) # no match : score = 0 (not supposed to happen in a real case usage of the scoring)
```

### Predict function
```{r}
# DESCRIPTION
#   Returns the most likely next words (with the associated score) based on the input n-gram
# INPUT
#   - inputNgram : n-gram with the format "W1_W2_W3_W4" where WX are words. 1 to 4 words are acceptable
#   - ngramModel : list of 4 elements containing data.tables of the DFM for the model.
#       The data.tables in the list have 2 columns : ngram and count
#       The first element of the list is the 1-gram DFM, the second is the 2-grams DFM etc.#       
# OUTPUT
#   - data.frame with 2 columns : (nextWord, score)
#     Maximum number of rows = MAX_NB_PREDICT and rows ordered with the highest scores first
#  
predictNextWords <- function(inputNgram, ngramModel) {
  ngramMatches <- getNgramMatch(inputNgram, lDfm)
  # TODO : careful not to have same prediction from different ngram levels (do before calling scoring)
  

  ngramMatches$score <- sapply(ngramMatches$ngram, getSboScore, ngramModel)
  
  lowerNgram <- unlist(strsplit(inputNgram, "_", fixed = TRUE))
          lowerNgram <- lowerNgram[2:length(lowerNgram)]
          lowerNgram <- paste(lowerNgram, collapse = "_")
          
    
  # Keep only last word of ngram    
  ngramMatches$nextWord <- sapply(ngramMatches$ngram, strsplit, "_", fixed = TRUE)
  for(i in seq_along(ngramMatches$nextWord)) {
    ngramMatches$nextWord[i] <- ngramMatches$nextWord[[i]][length(ngramMatches$nextWord[[i]])]
  }
  ngramMatches$nextWord <- unlist(ngramMatches$nextWord)
  
  # Remove duplicate next words from lower level n-grams
  ngramMatches <- ngramMatches[!duplicated(ngramMatches$nextWord)]
   
  # Returns only the MAX_NB_PREDICT n-grams with the highest scores
  head(ngramMatches[, nextWord, score][order(score, decreasing = TRUE)], MAX_NB_PREDICT)
}

# Tests from Quiz 2
predictNextWords("a_case_of", lDfm)
predictNextWords("would_mean_the", lDfm)
predictNextWords("make_me_the", lDfm)
predictNextWords("struggling_but_the", lDfm)
predictNextWords("date_at_the", lDfm)
predictNextWords("be_on_my", lDfm)
predictNextWords("in_quite_some", lDfm)
predictNextWords("with_his_little", lDfm)
predictNextWords("faith_during_the", lDfm)
predictNextWords("you_must_be", lDfm)
```


## TODO : n-gram pruning (distro of the DFM to see how to set MIN_NGRAM_COUNT and remove ngrams with low frequency)


## TODO : smoothing (= ???) and backoff (+ see Markov link to data table format)
