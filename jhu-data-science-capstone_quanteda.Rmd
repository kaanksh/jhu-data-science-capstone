---
title: "Coursera Data Science Capstone - Prediction model prototype with Quanteda"
author: "Samy Soualem"
date: "September 14, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document is just a technical memo for personal use.

```{r echo = FALSE, results = FALSE, message = FALSE}
library(quanteda)
library(ggplot2)
library(doParallel)
library(data.table)
library(stringr)

# PARAMETERS : modify if necessary
# If USE_SAMPLE_DATA is FALSE, all the data will be used
# If USE_SAMPLE_DATA is TRUE, only a random subset of FILE_SAMPLE_RATE will be used. 
USE_SAMPLE_DATA <- TRUE
# sample rate for the original files. This subset will be used to create the training / validation / test sets
FILE_SAMPLE_RATE <- 0.17
TRAINING_SAMPLE_RATE <- 0.6 # Rate for validation and test sets are the same
MAX_NB_PREDICT <- 5 # maximum number of predictions
MAX_N_INPUT_NGRAM <- 3  # maximum number of words for an input ngram
MAX_NGRAM_LEVEL <- 4
LAMBDA_SBO <- 0.4 # lambda parameter for stupid backoff
NB_CORES <- 4
TOP_NGRAM_LIMIT <- 50 # Number of ngrams to plot
# minimum number of occurences a ngram needs to have to be kept in the DFM
# For reference, the Google cut-off point was at least 40 occurences for 1,024,908,267,229 in total
MIN_NGRAM_COUNT <- 2
RDS_FILE_VERSION <- "v0.15"  # to know from which model a RSD file was created

SENT_START_MARKER <-"#s#"
  
registerDoParallel(cores=NB_CORES)

# Initialiaze list of DFM
lDfm <- vector(mode = "list", length = MAX_NGRAM_LEVEL)

source("model/trainModel.R")
source("model/predictWord.R")
```

```{r echo = FALSE, results = FALSE}
# Data source : https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip
# Data directories
enUsDir <- "data/final/en_US"
enUsSampleDir <- file.path(enUsDir, paste0("sample_", FILE_SAMPLE_RATE))
enFileNames <- c("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt")
enFile <- file.path(enUsDir, enFileNames)


fileSetsSuffix <- c("train.txt", "validation.txt", "test.txt")
blogSampleFile <- file.path(enUsSampleDir
                  , paste("en_US.blogs_sample", FILE_SAMPLE_RATE,   
                      fileSetsSuffix , sep = "_"
                    )
                  )
newsSampleFile <- file.path(enUsSampleDir
                  , paste("en_US.news_sample", FILE_SAMPLE_RATE,   
                      fileSetsSuffix , sep = "_"
                    )
                  )
twitterSampleFile <- file.path(enUsSampleDir
                  , paste("en_US.twitter_sample", FILE_SAMPLE_RATE,   
                      fileSetsSuffix , sep = "_"
                    )
                  )
                           
enSampleFile <- list(blogSampleFile, newsSampleFile, twitterSampleFile)

swearWordsFile <- "data/swearWords.csv"
rdsDir <- paste("data/rds", RDS_FILE_VERSION, paste("sample", FILE_SAMPLE_RATE, sep = '_') , sep = "/")
```

## RDS files preparation to save the results
```{r}
if (! dir.exists(rdsDir)) {
  dir.create(rdsDir, recursive = TRUE)
}

rdsSampleRate <- paste("sample", FILE_SAMPLE_RATE, sep ='_')
rdsSuffix <- ".rds"

# Version and sample rate in file name just in case
rdsFileSentences <- file.path(rdsDir, paste0(paste(RDS_FILE_VERSION, rdsSampleRate, "sentenceTokens", sep = '_'), rdsSuffix))
rdsFileDtDfm1 <- file.path(rdsDir, paste0(paste(RDS_FILE_VERSION, rdsSampleRate, "dtDfm1", sep = '_'), rdsSuffix))
rdsFileDtDfm2 <- file.path(rdsDir, paste0(paste(RDS_FILE_VERSION, rdsSampleRate, "dtDfm2", sep = '_'), rdsSuffix))
rdsFileDtDfm3 <- file.path(rdsDir, paste0(paste(RDS_FILE_VERSION, rdsSampleRate, "dtDfm3", sep = '_'), rdsSuffix))
rdsFileDtDfm4 <- file.path(rdsDir, paste0(paste(RDS_FILE_VERSION, rdsSampleRate, "dtDfm4", sep = '_'), rdsSuffix))
```

## Creating training, validation and test sets files

Total file sample rate : `r I(FILE_SAMPLE_RATE * 100)`%.

Training set ratio : `r I(TRAINING_SAMPLE_RATE * 100)`%

Validation set ratio : `r I((1 - TRAINING_SAMPLE_RATE) / 2 * 100)`%

Test set ratio : `r I((1 - TRAINING_SAMPLE_RATE) / 2 * 100)`%

```{r echo = FALSE, results = FALSE}
# Stop with an error if the original files are not found
if (! all(file.exists(enFile))) {
  errMsg <- paste("The following files were not found :", paste(enFile, collapse = ", "))
  stop(errMsg)
}


if (USE_SAMPLE_DATA) {
  # If all the sample files don't already exist for this sample rate, create them
  if (! all(file.exists(unlist(enSampleFile)))) {
    if (! dir.exists(enUsSampleDir)) {
      dir.create(enUsSampleDir)
    }
    
    # Read all files, generate a random sample and save it to another file for later use
    set.seed(1234)
    for(i in 1:length(enFile)) {
      tempData <- readLines(enFile[i])
      # Only keep a random subset before creating the training, validation and test sets
      # Done because files are large and we want the validation and test sets to have held out samples
      tempData <- sample(tempData, as.integer(FILE_SAMPLE_RATE * length(tempData)))
      
      # Training set
      inTraining <- seq_along(tempData)
      inTraining <- sample(inTraining, as.integer(TRAINING_SAMPLE_RATE * length(inTraining)))
      trainSet <- tempData[inTraining]
      
      # Validation and test sets
      validAndTestSet <- tempData[-inTraining]
      inTest <- seq_along(validAndTestSet)
      inTest <- sample(inTest, as.integer(0.5 * length(inTest)))  # 0.5 b/c validation and test sets have same size
      testSet <- validAndTestSet[inTest]
      validSet <- validAndTestSet[-inTest]
      
      # Save sampled datasets in files
      writeLines(trainSet, enSampleFile[[i]][1])
      writeLines(validSet, enSampleFile[[i]][2])
      writeLines(testSet, enSampleFile[[i]][3])
    }
    # Free memory (not very useful in general apparently)
    rm(tempData)
    rm(trainSet)
    rm(validSet)
    rm(testSet)
    gc()  # garbage collector
  }
  # Sample files here, nothing to do
}
```


## Load training set into a Corpus
```{r cache = TRUE, echo = FALSE, results = FALSE}
# Only load Corpus if the tokenized sentences don't exist in a RDS file for this version and sample rate
if(!file.exists(rdsFileSentences)) {
  if (USE_SAMPLE_DATA) {
    corpusFiles <- character()
    
    # For each document, only load the training set
    for(i in 1:length(enSampleFile)) {
      corpusFiles <- c(corpusFiles, enSampleFile[[i]][1]) # Training set always first
    }
  } else {
    corpusFiles <- enFile
  }
  # Load documents as a Quanteda Corpus
  enRawCorpus <- corpus(textfile(corpusFiles, encoding = "UTF-8"))
  #summary(enRawCorpus)
}
```



## Tokenizing
### Sentences
```{r cache = TRUE}
# If a previous RDS file exists for this version and sample rate, load it
# Else compute the tokens and save it to a RDS file
if(file.exists(rdsFileSentences)) {
  sentenceTokens <- readRDS(rdsFileSentences)
} else {
  # Always tokenize sentences first
  #sentenceTokens <- tokenizeSentences(enRawCorpus)
  system.time(sentenceTokens <- tokenizeSentences(enRawCorpus))
  saveRDS(sentenceTokens, rdsFileSentences)
  
  # Free memory (not very useful in general apparently)
  rm(enRawCorpus)
  gc()
}
```

### 1-grams
Features are ignored in the dfm step to avoid creating wrong ngrams when n > 1 (because of a word being removed from a ngram).

See the "Details" section of dfm's help page.
```{r cache = TRUE}
# If a previous RDS file exists for this version and sample rate, load it
# Else compute the tokens and save it to a RDS file
if(file.exists(rdsFileDtDfm1)) {
  lDfm[[1]] <- readRDS(rdsFileDtDfm1)
} else {
  # Ignore start of sentence markers (#s#) to avoid predicting it
  system.time(dfm1 <- generateDfm(sentenceTokens, 1, c(getSwearWords(), SENT_START_MARKER)))
  # 1.1 Gb for 15%
  
  
  # Convert DFM to data table format (faster and uses less memory)
  # Blank prefix for unigrams
  lDfm[[1]] <- data.table(prefix = "", lastWord = features(dfm1), count = as.integer(colSums(dfm1)), key = "count")
  saveRDS(lDfm[[1]], rdsFileDtDfm1)
  
  # Free memory (not very useful in general apparently)
  rm(dfm1)
  gc()
}
```

### 2-grams
```{r cache = TRUE}
# If a previous RDS file exists for this version and sample rate, load it
# Else compute the tokens and save it to a RDS file
if(file.exists(rdsFileDtDfm2)) {
  lDfm[[2]] <- readRDS(rdsFileDtDfm2)
} else {
  system.time(dfm2 <- generateDfm(sentenceTokens, 2, getSwearWords()))
  
  # Split ngrams' last word from the prefix
  tempSplitPrefix <- t(vapply(features(dfm2), splitNgramPrefix, c("", "")))
  
  # Convert DFM to data table format (faster and uses less memory)
  lDfm[[2]] <- data.table(prefix = tempSplitPrefix[, 1], lastWord = tempSplitPrefix[ ,2]
                          , count = as.integer(colSums(dfm2)), key = c("prefix", "count"))
  saveRDS(lDfm[[2]], rdsFileDtDfm2)
  
  # Free memory (not very useful in general apparently)
  rm(dfm2)
  gc()
}
```

### 3-grams
```{r cache = TRUE}
# If a previous RDS file exists for this version and sample rate, load it
# Else compute the tokens and save it to a RDS file
if(file.exists(rdsFileDtDfm3)) {
  lDfm[[3]] <- readRDS(rdsFileDtDfm3)
} else {
  system.time(dfm3 <- generateDfm(sentenceTokens, 3, getSwearWords()))
  # 1.5 Gb for 15%
  
  # Split ngrams' last word from the prefix
  tempSplitPrefix <- t(vapply(features(dfm3), splitNgramPrefix, c("", "")))
  
  # Convert DFM to data table format (faster and uses less memory)
  lDfm[[3]] <- data.table(prefix = tempSplitPrefix[, 1], lastWord = tempSplitPrefix[ ,2]
                          , count = as.integer(colSums(dfm3)), key = c("prefix", "count"))
  saveRDS(lDfm[[3]], rdsFileDtDfm3)
  
  # Free memory (not very useful in general apparently)
  rm(dfm3)
  gc()
}
```

### 4-grams
```{r cache = TRUE}
# If a previous RDS file exists for this version and sample rate, load it
# Else compute the tokens and save it to a RDS file
if(file.exists(rdsFileDtDfm4)) {
  lDfm[[4]] <- readRDS(rdsFileDtDfm4)
} else {
  system.time(dfm4 <- generateDfm(sentenceTokens, 4, getSwearWords()))
  # 1.7 Gb for 15%
  # 6325s and 2 GB for lDfm (all DFM as data.tables) for FILE_SAMPLE_RATE = 0.3
  # 818s for FILE_SAMPLE_RATE = 0.17 et TRAINING_SAMPLE_RATE = 0.6
  
  tempSplitPrefix <- t(vapply(features(dfm4), splitNgramPrefix, c("", "")))
  
  # Convert DFM to data table format (faster and uses less memory)
  lDfm[[4]] <- data.table(prefix = tempSplitPrefix[, 1], lastWord = tempSplitPrefix[ ,2]
                          , count = as.integer(colSums(dfm4)), key = c("prefix", "count"))
  saveRDS(lDfm[[4]], rdsFileDtDfm4)
  
  # Free memory (not very useful in general apparently)
  rm(dfm4)
  gc()
}
```

## Remove low-frequency n-grams to reduce model size and lower response time
```{r}
lDfmTrim <- vector(mode = "list", length = length(lDfm))
# TODO : when final value of MIN_NGRAM_COUNT set, note actual savings (number of lines and %)
# Delete low frequency ngram
for(i in 1:length(lDfm)) {
  lDfmTrim[[i]] <- lDfm[[i]][ count >= MIN_NGRAM_COUNT, ]
}
```
