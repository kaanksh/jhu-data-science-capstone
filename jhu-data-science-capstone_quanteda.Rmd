---
title: "Coursera Data Science Capstone - Prediction model prototype with Quanteda"
author: "Samy Soualem"
date: "September 14, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document is just a technical memo for personal use.

```{r echo = FALSE, results = FALSE, message = FALSE}
library(quanteda)
library(ggplot2)
library(doParallel)
library(data.table)
library(stringr)

# PARAMETERS : modify if necessary
# If USE_SAMPLE_DATA is FALSE, all the data will be used
# If USE_SAMPLE_DATA is TRUE, only a random subset of FILE_SAMPLE_RATE will be used. 
USE_SAMPLE_DATA <- TRUE
# sample rate for the original files. This subset will be used to create the training / validation / test sets
FILE_SAMPLE_RATE <- 0.01
TRAINING_SAMPLE_RATE <- 0.6 # Rate for validation and test sets are the same
MAX_NB_PREDICT <- 5 # maximum number of predictions
MAX_N_INPUT_NGRAM <- 3  # maximum number of words for an input ngram
MAX_NGRAM_LEVEL <- 4
LAMBDA_SBO <- 0.4 # lambda parameter for stupid backoff
NB_CORES <- 4
TOP_NGRAM_LIMIT <- 50 # Number of ngrams to plot
# minimum number of occurences a ngram needs to have to be kept in the DFM
# For reference, the Google cut-off point was at least 40 occurences for 1,024,908,267,229 in total
MIN_NGRAM_COUNT <- 2
RDS_FILE_VERSION <- "v0.13"  # to know from which model a RSD file was created

registerDoParallel(cores=NB_CORES)

# Initialiaze list of DFM
lDfm <- vector(mode = "list", length = MAX_NGRAM_LEVEL)
```

```{r echo = FALSE, results = FALSE}
# Data source : https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip
# Data directories
enUsDir <- "data/final/en_US"
enUsSampleDir <- file.path(enUsDir, paste0("sample_", FILE_SAMPLE_RATE))
enFileNames <- c("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt")
enFile <- file.path(enUsDir, enFileNames)


fileSetsSuffix <- c("train.txt", "validation.txt", "test.txt")
blogSampleFile <- file.path(enUsSampleDir
                  , paste("en_US.blogs_sample", FILE_SAMPLE_RATE,   
                      fileSetsSuffix , sep = "_"
                    )
                  )
newsSampleFile <- file.path(enUsSampleDir
                  , paste("en_US.news_sample", FILE_SAMPLE_RATE,   
                      fileSetsSuffix , sep = "_"
                    )
                  )
twitterSampleFile <- file.path(enUsSampleDir
                  , paste("en_US.twitter_sample", FILE_SAMPLE_RATE,   
                      fileSetsSuffix , sep = "_"
                    )
                  )
                           
enSampleFile <- list(blogSampleFile, newsSampleFile, twitterSampleFile)

swearWordsFile <- "data/swearWords.csv"
rdsDir <- paste("data/rds", RDS_FILE_VERSION, paste("sample", FILE_SAMPLE_RATE, sep = '_') , sep = "/")
```

## RDS files preparation to save the results
```{r}
if (! dir.exists(rdsDir)) {
  dir.create(rdsDir, recursive = TRUE)
}

rdsSampleRate <- paste("sample", FILE_SAMPLE_RATE, sep ='_')
rdsSuffix <- ".rds"

# Version and sample rate in file name just in case
rdsFileSentences <- file.path(rdsDir, paste0(paste(RDS_FILE_VERSION, rdsSampleRate, "sentenceTokens", sep = '_'), rdsSuffix))
rdsFileDtDfm1 <- file.path(rdsDir, paste0(paste(RDS_FILE_VERSION, rdsSampleRate, "dtDfm1", sep = '_'), rdsSuffix))
rdsFileDtDfm2 <- file.path(rdsDir, paste0(paste(RDS_FILE_VERSION, rdsSampleRate, "dtDfm2", sep = '_'), rdsSuffix))
rdsFileDtDfm3 <- file.path(rdsDir, paste0(paste(RDS_FILE_VERSION, rdsSampleRate, "dtDfm3", sep = '_'), rdsSuffix))
rdsFileDtDfm4 <- file.path(rdsDir, paste0(paste(RDS_FILE_VERSION, rdsSampleRate, "dtDfm4", sep = '_'), rdsSuffix))
```

## Creating training, validation and test sets files

Total file sample rate : `r I(FILE_SAMPLE_RATE * 100)`%.

Training set ratio : `r I(TRAINING_SAMPLE_RATE * 100)`%

Validation set ratio : `r I((1 - TRAINING_SAMPLE_RATE) / 2 * 100)`%

Test set ratio : `r I((1 - TRAINING_SAMPLE_RATE) / 2 * 100)`%

```{r echo = FALSE, results = FALSE}
# Stop with an error if the original files are not found
if (! all(file.exists(enFile))) {
  errMsg <- paste("The following files were not found :", paste(enFile, collapse = ", "))
  stop(errMsg)
}


if (USE_SAMPLE_DATA) {
  # If all the sample files don't already exist for this sample rate, create them
  if (! all(file.exists(unlist(enSampleFile)))) {
    if (! dir.exists(enUsSampleDir)) {
      dir.create(enUsSampleDir)
    }
    
    # Read all files, generate a random sample and save it to another file for later use
    set.seed(1234)
    for(i in 1:length(enFile)) {
      tempData <- readLines(enFile[i])
      # Only keep a random subset before creating the training, validation and test sets
      # Done because files are large and we want the validation and test sets to have held out samples
      tempData <- sample(tempData, as.integer(FILE_SAMPLE_RATE * length(tempData)))
      
      # Training set
      inTraining <- seq_along(tempData)
      inTraining <- sample(inTraining, as.integer(TRAINING_SAMPLE_RATE * length(inTraining)))
      trainSet <- tempData[inTraining]
      
      # Validation and test sets
      validAndTestSet <- tempData[-inTraining]
      inTest <- seq_along(validAndTestSet)
      inTest <- sample(inTest, as.integer(0.5 * length(inTest)))  # 0.5 b/c validation and test sets have same size
      testSet <- validAndTestSet[inTest]
      validSet <- validAndTestSet[-inTest]
      
      # Save sampled datasets in files
      writeLines(trainSet, enSampleFile[[i]][1])
      writeLines(validSet, enSampleFile[[i]][2])
      writeLines(testSet, enSampleFile[[i]][3])
    }
    # Free memory (not very useful in general apparently)
    rm(tempData)
    rm(trainSet)
    rm(validSet)
    rm(testSet)
    gc()  # garbage collector
  }
  # Sample files here, nothing to do
}
```


## Load training set into a Corpus
```{r cache = TRUE, echo = FALSE, results = FALSE}
# Only load Corpus if the tokenized sentences don't exist in a RDS file for this version and sample rate
if(!file.exists(rdsFileSentences)) {
  if (USE_SAMPLE_DATA) {
    corpusFiles <- character()
    
    # For each document, only load the training set
    for(i in 1:length(enSampleFile)) {
      corpusFiles <- c(corpusFiles, enSampleFile[[i]][1]) # Training set always first
    }
  } else {
    corpusFiles <- enFile
  }
  # Load documents as a Quanteda Corpus
  enRawCorpus <- corpus(textfile(corpusFiles, encoding = "UTF-8"))
  #summary(enRawCorpus)
}
```

## Tokenizing functions
The corpus should be tokenized as sentences before creating the n-grams tokens. This makes sure that no n-gram will be created with the end of sentence and the beginning of the next.
```{r results = FALSE}
# Get vector of swear words to remove
# File downloaded from http://www.bannedwordlist.com/lists/swearWords.csv
getSwearWords <- function() {
  swearWords <- readLines(swearWordsFile, skipNul = TRUE, warn = FALSE) # warning because of missing "end of line" character
  swearWords <- unlist(strsplit(swearWords, ",", fixed = TRUE))
  swearWords
}

# Sentence tokenization function
tokenizeSentences <- function(x) {
  # TODO : remove twitter characters here manually
  
  tokens <- tokenize(x, what = "sentence", removeNumbers = TRUE, removePunct = TRUE, removeSymbols = TRUE
                     , removeSeparators = TRUE, removeTwitter = FALSE, removeHyphen = TRUE, removeURL = TRUE)
  
  # Add start and end of sentences symbols
  # #bos# = beginning of sentence
  # #eos# = end of sentence (like EOF symbol for "end of file")
  
  # Symbols added because of the Ngrams chapter in the book Speech and Language Processing
  # by Daniel Jurafsky and James Martin
  # Not sure if these symbols will be used in the prediction model
  # WARNING : these symbols cannot use punctuation that would be removed in the n-grams tokenization
  
  # ************************************************************************************
  # TODO : see if start and end of sentences useful for final model.
  # => If used : 
  #       - set removeTwitter to FALSE in the tokenizeNgrams function
  #       - Remove Twitter content with regexp manually after the ngram tokenization
  # ************************************************************************************
  #unlist(lapply(tokens, function(y) paste('#bos#', toLower(y), '#eos#')))
  unlist(lapply(tokens, toLower))
}

# N-grams tokenization function
# Default is unigram if n is not set
# removeTwitter needs to be FALSE in case start and end of sentence markers used (# character)
tokenizeNgrams <- function(x, n = 1L) {
  tokenize(x, what = "word", removeNumbers = TRUE, removePunct = TRUE, removeSymbols = TRUE
           , removeSeparators = TRUE, removeTwitter = TRUE, removeHyphen = TRUE, removeURL = TRUE
           , ngrams = n, simplify = TRUE)
}

# NEW FUNCTIONS
getNgramPrefix <- function(string, sep = "_") {
  if(length(string) != 1) {
    stop("Only atomic vectors allowed")
  }
  
  splitNgram <- strsplit(string, "_", fixed = TRUE)[[1]]
  paste0(splitNgram[1:length(splitNgram) - 1], collapse = "_")
}

getNgramLastWord <- function(string, sep = "_") {
  if(length(string) != 1) {
    stop("Only atomic vectors allowed")
  }
  
  splitNgram <- strsplit(string, "_", fixed = TRUE)[[1]]
  splitNgram[length(splitNgram)]
}

splitNgramPrefix <- function(string, sep = "_") {
  if(length(string) != 1) {
    stop("Only atomic vectors allowed")
  }
  
  splitNgram <- strsplit(string, "_", fixed = TRUE)[[1]]
  n <- length(splitNgram) 
  c(paste0(splitNgram[1:n- 1], collapse = "_"), splitNgram[n])
}
```



## Tokenizing
### Sentences
```{r cache = TRUE}
# If a previous RDS file exists for this version and sample rate, load it
# Else compute the tokens and save it to a RDS file
if(file.exists(rdsFileSentences)) {
  sentenceTokens <- readRDS(rdsFileSentences)
} else {
  # Always tokenize sentences first
  #sentenceTokens <- tokenizeSentences(enRawCorpus)
  system.time(sentenceTokens <- tokenizeSentences(enRawCorpus))
  saveRDS(sentenceTokens, rdsFileSentences)
  
  # Free memory (not very useful in general apparently)
  rm(enRawCorpus)
  gc()
}
```

### 1-grams
Features are ignored in the dfm step to avoid creating wrong ngrams when n > 1 (because of a word being removed from a ngram).

See the "Details" section of dfm's help page.
```{r cache = TRUE}
# If a previous RDS file exists for this version and sample rate, load it
# Else compute the tokens and save it to a RDS file
if(file.exists(rdsFileDtDfm1)) {
  lDfm[[1]] <- readRDS(rdsFileDtDfm1)
} else {
  #ngram1 <- tokenizeNgrams(sentenceTokens, 1)
  system.time(ngram1 <- tokenizeNgrams(sentenceTokens, 1))
  #dfm1 <- dfm(ngram1, ignoredFeatures=getSwearWords())
  system.time(dfm1 <- dfm(ngram1, ignoredFeatures=getSwearWords()))
  # 1.1 Gb for 15%
  
  # Convert DFM to data table format (faster and uses less memory)
  # Blank prefix for unigrams
  lDfm[[1]] <- data.table(prefix = "", lastWord = features(dfm1), count = as.integer(colSums(dfm1)), key = "lastWord")
  saveRDS(lDfm[[1]], rdsFileDtDfm1)
  
  # Free memory (not very useful in general apparently)
  rm(ngram1)
  rm(dfm1)
  gc()
}
```

### 2-grams
```{r cache = TRUE}
# If a previous RDS file exists for this version and sample rate, load it
# Else compute the tokens and save it to a RDS file
if(file.exists(rdsFileDtDfm2)) {
  lDfm[[2]] <- readRDS(rdsFileDtDfm2)
} else {
  #ngram2 <- tokenizeNgrams(sentenceTokens, 2)
  system.time(ngram2 <- tokenizeNgrams(sentenceTokens, 2))
  #dfm2 <- dfm(ngram2, ignoredFeatures=getSwearWords())
  system.time(dfm2 <- dfm(ngram2, ignoredFeatures=getSwearWords()))
  
  # Split ngrams' last word from the prefix
  tempSplitPrefix <- t(vapply(features(dfm2), splitNgramPrefix, c("", "")))
  
  # Convert DFM to data table format (faster and uses less memory)
  lDfm[[2]] <- data.table(prefix = tempSplitPrefix[, 1], lastWord = tempSplitPrefix[ ,2]
                          , count = as.integer(colSums(dfm2)), key = "prefix")
  
  saveRDS(lDfm[[2]], rdsFileDtDfm2)
  
  # Free memory (not very useful in general apparently)
  rm(ngram2)
  rm(dfm2)
  gc()
}
```

### 3-grams
```{r cache = TRUE}
# If a previous RDS file exists for this version and sample rate, load it
# Else compute the tokens and save it to a RDS file
if(file.exists(rdsFileDtDfm3)) {
  lDfm[[3]] <- readRDS(rdsFileDtDfm3)
} else {
  #ngram3 <- tokenizeNgrams(sentenceTokens, 3)
  system.time(ngram3 <- tokenizeNgrams(sentenceTokens, 3))
  #dfm3 <- dfm(ngram3, ignoredFeatures=getSwearWords())
  system.time(dfm3 <- dfm(ngram3, ignoredFeatures=getSwearWords()))
  # 1.5 Gb for 15%
  
  # Split ngrams' last word from the prefix
  tempSplitPrefix <- t(vapply(features(dfm3), splitNgramPrefix, c("", "")))
  
  # Convert DFM to data table format (faster and uses less memory)
  lDfm[[3]] <- data.table(prefix = tempSplitPrefix[, 1], lastWord = tempSplitPrefix[ ,2]
                          , count = as.integer(colSums(dfm3)), key = "prefix")
  saveRDS(lDfm[[3]], rdsFileDtDfm3)
  
  # Free memory (not very useful in general apparently)
  rm(ngram3)
  rm(dfm3)
  gc()
}
```

### 4-grams
```{r cache = TRUE}
# If a previous RDS file exists for this version and sample rate, load it
# Else compute the tokens and save it to a RDS file
if(file.exists(rdsFileDtDfm4)) {
  lDfm[[4]] <- readRDS(rdsFileDtDfm4)
} else {
  #ngram4 <- tokenizeNgrams(sentenceTokens, 4)
  system.time(ngram4 <- tokenizeNgrams(sentenceTokens, 4))
  #dfm4 <- dfm(ngram4, ignoredFeatures=getSwearWords())
  system.time(dfm4 <- dfm(ngram4, ignoredFeatures=getSwearWords()))
  # 1.7 Gb for 15%
  # 6325s and 2 GB for lDfm (all DFM as data.tables) for FILE_SAMPLE_RATE = 0.3
  
  tempSplitPrefix <- t(vapply(features(dfm4), splitNgramPrefix, c("", "")))
  
  # Convert DFM to data table format (faster and uses less memory)
  lDfm[[4]] <- data.table(prefix = tempSplitPrefix[, 1], lastWord = tempSplitPrefix[ ,2]
                          , count = as.integer(colSums(dfm4)), key = "prefix")
  saveRDS(lDfm[[4]], rdsFileDtDfm4)
  
  # Free memory (not very useful in general apparently)
  rm(ngram4)
  rm(dfm4)
  gc()
}
```



## TODO : Prediction prototype
### Get n-gram containing the next-word candidates
```{r}
# DESCRIPTION
#   Returns n-grams containing the next-word candidates for the input n-gram
# INPUT
#   - inputNgram : n-gram with the format "W1_W2_W3" where WX are words. 1 to 3 words are acceptable
#   - ngramModel : list of 4 elements containing data.tables of the DFM for the model.
#       The data.tables in the list have 2 columns : ngram and count
#       The first element of the list is the 1-gram DFM, the second is the 2-grams DFM etc.#       
# OUTPUT
#   - data.table(ngram, count) :
#       Contains at most MAX_N_INPUT_NGRAM n-grams with their count number
#       If no match is found the output is empty
#       
getNgramMatch <- function(inputNgram, ngramModel) {
  if(length(inputNgram) != 1) {
    stop("Only atomic vectors allowed")
  }
  
  # Default empty result
  match <- data.table(prefix = character(0), lastWord = character(0), count = integer(0))
  
  # Input n-gram level
  n <- length(strsplit(inputNgram, "_", fixed = TRUE)[[1]])
  
  if(n > MAX_N_INPUT_NGRAM) {
    errorMsg <- paste("The input n-gram", paste0("\"", inputNgram, "\""), "has more than", MAX_N_INPUT_NGRAM, "words.")
    stop(errorMsg)
  }
 
  # n+1 because if input is a trigram, lookup needs to be done in the fourgram model
  # Return at most MAX_NB_PREDICT of the most frequent matches
  match <- head(ngramModel[[n+1]][prefix == inputNgram][order(count, decreasing = TRUE)], MAX_NB_PREDICT)
  
  # Less than MAX_NB_PREDICT matches found : try to match a lower-level ngram
  if(nrow(match) < MAX_NB_PREDICT) {
    if(n >=2) {
      # If at least 2 words, remove FIRST word and try to match this new ngram
      croppedNgram <- str_split_fixed(inputNgram, "_", 2)[2]
      # Add lower-level ngrams to the current level ngrams
      # fill = TRUE because of 1-grams which don't have a prefix
      match <- rbind(match, getNgramMatch(croppedNgram, ngramModel))
    # If not enough matches to reach MAX_NB_PREDICT : return most common unigrams to complete the set of predictions
    } else {
      # Add lower-level ngrams to the current level ngrams
      match <- rbind(match, head(ngramModel[[1]][order(count, decreasing = TRUE)], MAX_NB_PREDICT))
    }
  }
  match
}
```

### Scoring function (stupid backoff)
```{r}
# DESCRIPTION
#   Returns the stupid backoff score of the input n-gram
# INPUT
#   - inputNgram : n-gram with the format "W1_W2_W3_W4" where WX are words. 1 to 4 words are acceptable
#   - ngramModel : list of 4 elements containing data.tables of the DFM for the model.
#       The data.tables in the list have 2 columns : ngram and count
#       The first element of the list is the 1-gram DFM, the second is the 2-grams DFM etc.#       
# OUTPUT
#   - Stupid backoff score for inputNgram (numeric)
#  
getSboScore <- function(inputNgram, ngramModel) {
  if(length(inputNgram) != 1) {
    stop("Only atomic vectors allowed")
  }
  
  # Default result if ngram not found
  score <- 0
  
  # TODO : A LOT of perf optim to do because counts sometimes recalculated for nothing
  
  # Input n-gram level
  n <- length(strsplit(inputNgram, "_", fixed = TRUE)[[1]])
  
  # 0-word ngrams
  if(n <= 0) {
    score <- 0
  # At least 1 word
  } else {
      tempSplitPrefix <- splitNgramPrefix(inputNgram)
      inputNgramCount <- ngramModel[[n]][prefix == tempSplitPrefix[1] & lastWord == tempSplitPrefix[2]]$count
      
      # Stop with error if more than 1 match (duplicates problem)
      if((length(inputNgramCount) > 1)) {
        errorMsg <- paste("Duplicate rows found for ngram", inputNgram)
        stop(errorMsg)
      }
      
      # If ngram has at least 1 occurence (i.e. : 1 match in the ngram model)
      if(length(inputNgramCount) == 1) {
        # For unigrams, lower ngram count = number of observed unigrams
        if(n == 1) {
          lowerNgramCount <- nrow(ngramModel[[n]])
        # n >= 2
        } else {
          # Remove LAST word to get the lower level ngram i.e use the ngram prefix
          # Prefix is blank ("") for unigrams
          lowerNSplitPrefix <- splitNgramPrefix(tempSplitPrefix[1])
          lowerNgramCount <- ngramModel[[n - 1]][prefix == lowerNSplitPrefix[1] & lastWord == lowerNSplitPrefix[2]]$count
        }
        score <- inputNgramCount / lowerNgramCount
      # ngram has no occurence
      } else {
        if(n >= 2) {
          # Remove FIRST word to get the lower level ngram
          croppedNgram <- str_split_fixed(inputNgram, "_", 2)[2]
          score <- LAMBDA_SBO * getSboScore(croppedNgram, ngramModel)
        }
      } 
  }
  score
}
```

### Predict function
```{r}
# DESCRIPTION
#   Returns the most likely next words (with the associated score) based on the input n-gram
# INPUT
#   - inputNgram : n-gram with the format "W1_W2_W3_W4" where WX are words. 1 to 4 words are acceptable
#   - ngramModel : list of 4 elements containing data.tables of the DFM for the model.
#       The data.tables in the list have 2 columns : ngram and count
#       The first element of the list is the 1-gram DFM, the second is the 2-grams DFM etc.#       
# OUTPUT
#   - data.frame with 2 columns : (nextWord, score)
#     Maximum number of rows = MAX_NB_PREDICT and rows ordered with the highest scores first
#  
predictNextWord <- function(inputNgram, ngramModel) {
  ngramMatches <- getNgramMatch(inputNgram, ngramModel)

  fullNgram <- paste(ngramMatches$prefix, ngramMatches$lastWord, sep = "_")
  ngramMatches$score <- sapply(fullNgram, getSboScore, ngramModel)
  
  # Remove FIRST word to get the lower level ngram
  croppedNgram <- str_split_fixed(inputNgram, "_", 2)[2]
  
  # Remove duplicate next words from lower level n-grams
  ngramMatches <- ngramMatches[!duplicated(ngramMatches$lastWord)]
   
  # Returns only the MAX_NB_PREDICT n-grams with the highest scores
  head(ngramMatches[, lastWord, score][order(score, decreasing = TRUE)], MAX_NB_PREDICT)
}
```


## Remove low-frequency n-grams to reduce model size and lower response time
```{r}
lDfmOptim <- lDfm
# TODO : when final value of MIN_NGRAM_COUNT set, note actual savings (number of lines and %)
# Low frequence ngram pruning
for(i in 1:length(lDfm)) {
  lDfmOptim[[i]] <- lDfm[[i]][ count >= MIN_NGRAM_COUNT, ]
}
```


## Analysing the most common n-grams
### Keep only the `r I(TOP_NGRAM_LIMIT)` most common n-grams for each document-term matrix
```{r}
dtFreq1Top <- head(lDfmOptim[[1]][order(count, decreasing = TRUE)], TOP_NGRAM_LIMIT)
dtFreq2Top <- head(lDfmOptim[[2]][order(count, decreasing = TRUE)], TOP_NGRAM_LIMIT)
dtFreq3Top <- head(lDfmOptim[[3]][order(count, decreasing = TRUE)], TOP_NGRAM_LIMIT)
dtFreq4Top <- head(lDfmOptim[[4]][order(count, decreasing = TRUE)], TOP_NGRAM_LIMIT)
```

### Plot the most common n-grams
```{r}
# Most common 1-grams plot
g1 <- ggplot(dtFreq1Top, aes(x = reorder(ngram, -count), y = count))
g1 <- g1 + geom_bar(stat="identity", fill = "deepskyblue4")
g1 <- g1 + ggtitle("Top unigrams sorted by frequency")
g1 <- g1 + xlab("Unigram") + ylab("Count")
g1 <- g1 + theme(axis.text.x=element_text(angle=45, hjust=1))
g1

# Most common 2-grams plot
g2 <- ggplot(dtFreq2Top, aes(x = reorder(ngram, -count), y = count))
g2 <- g2 + geom_bar(stat="identity", fill = "deepskyblue4")
g2 <- g2 + ggtitle("Top bigrams sorted by frequency")
g2 <- g2 + xlab("Bigram") + ylab("Count")
g2 <- g2 + theme(axis.text.x=element_text(angle=45, hjust=1))
g2

# Most common 3-grams plot
g3 <- ggplot(dtFreq3Top, aes(x = reorder(ngram, -count), y = count))
g3 <- g3 + geom_bar(stat="identity", fill = "deepskyblue4")
g3 <- g3 + ggtitle("Top trigrams sorted by frequency")
g3 <- g3 + xlab("Trigram") + ylab("Count")
g3 <- g3 + theme(axis.text.x=element_text(angle=45, hjust=1))
g3

# Most common 4-grams plot
g4 <- ggplot(dtFreq4Top, aes(x = reorder(ngram, -count), y = count))
g4 <- g4 + geom_bar(stat="identity", fill = "deepskyblue4")
g4 <- g4 + ggtitle("Top fourgrams sorted by frequency")
g4 <- g4 + xlab("Fourgram") + ylab("Count")
g4 <- g4 + theme(axis.text.x=element_text(angle=45, hjust=1))
g4
```
