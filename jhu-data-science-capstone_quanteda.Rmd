---
title: "Coursera Data Science Capstone - Prediction model prototype with Quanteda"
author: "Samy Soualem"
date: "September 14, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document is just a technical memo for personal use.

```{r echo = FALSE, results = FALSE, message = FALSE}
library(quanteda)
library(ggplot2)
library(doParallel)
library(data.table)
library(caret)

# PARAMETERS : modify if necessary
# If USE_SAMPLE_DATA is FALSE, all the data will be used
# If USE_SAMPLE_DATA is TRUE, only a random subset of FILE_SAMPLE_RATE will be used. 
USE_SAMPLE_DATA <- TRUE
# sample rate for the original files. This subset will be used to create the training / validation / test sets
FILE_SAMPLE_RATE <- 0.01
TRAINING_SAMPLE_RATE <- 0.6 # Rate for validation and test sets are the same
NB_CORES <- 4
TOP_NGRAM_LIMIT <- 50 # Number of ngrams to plot
MIN_NGRAM_COUNT <- 2  # minimum number of occurences a ngram needs to have to be kept in the DFM 
RDS_FILE_VERSION <- "v0.11"  # to know from which model a RSD file was created

registerDoParallel(cores=NB_CORES)
```

```{r echo = FALSE, results = FALSE}
# Data source : https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip
# Data directories
enUsDir <- "data/final/en_US"
enUsSampleDir <- file.path(enUsDir, paste0("sample_", FILE_SAMPLE_RATE))
enFileNames <- c("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt")
enFile <- file.path(enUsDir, enFileNames)


fileSetsSuffix <- c("train.txt", "validation.txt", "test.txt")
blogSampleFile <- file.path(enUsSampleDir
                  , paste("en_US.blogs_sample", FILE_SAMPLE_RATE,   
                      fileSetsSuffix , sep = "_"
                    )
                  )
newsSampleFile <- file.path(enUsSampleDir
                  , paste("en_US.news_sample", FILE_SAMPLE_RATE,   
                      fileSetsSuffix , sep = "_"
                    )
                  )
twitterSampleFile <- file.path(enUsSampleDir
                  , paste("en_US.twitter_sample", FILE_SAMPLE_RATE,   
                      fileSetsSuffix , sep = "_"
                    )
                  )
                           
enSampleFile <- list(blogSampleFile, newsSampleFile, twitterSampleFile)

swearWordsFile <- "data/swearWords.csv"
rdsDir <- paste("data/rds", RDS_FILE_VERSION, paste("sample", FILE_SAMPLE_RATE, sep = '_') , sep = "/")
```

## RDS files preparation to save the results
```{r}
if (! dir.exists(rdsDir)) {
  dir.create(rdsDir, recursive = TRUE)
}

rdsSampleRate <- paste("sample", FILE_SAMPLE_RATE, sep ='_')
rdsSuffix <- ".rds"

# Version and sample rate in file name just in case
rdsFileSentences <- file.path(rdsDir, paste0(paste(RDS_FILE_VERSION, rdsSampleRate, "sentenceTokens", sep = '_'), rdsSuffix))
rdsFileDtDfm1 <- file.path(rdsDir, paste0(paste(RDS_FILE_VERSION, rdsSampleRate, "dtDfm1", sep = '_'), rdsSuffix))
rdsFileDtDfm2 <- file.path(rdsDir, paste0(paste(RDS_FILE_VERSION, rdsSampleRate, "dtDfm2", sep = '_'), rdsSuffix))
rdsFileDtDfm3 <- file.path(rdsDir, paste0(paste(RDS_FILE_VERSION, rdsSampleRate, "dtDfm3", sep = '_'), rdsSuffix))
rdsFileDtDfm4 <- file.path(rdsDir, paste0(paste(RDS_FILE_VERSION, rdsSampleRate, "dtDfm4", sep = '_'), rdsSuffix))
```

## Creating training, validation and test sets files

Total file sample rate : `r I(FILE_SAMPLE_RATE * 100)`%.

Training set ratio : `r I(TRAINING_SAMPLE_RATE * 100)`%

Validation set ratio : `r I((1 - TRAINING_SAMPLE_RATE) / 2 * 100)`%

Test set ratio : `r I((1 - TRAINING_SAMPLE_RATE) / 2 * 100)`%

```{r echo = FALSE, results = FALSE}
# Stop with an error if the original files are not found
if (! all(file.exists(enFile))) {
  errMsg <- paste("The following files were not found :", paste(enFile, collapse = ", "))
  stop(errMsg)
}


if (USE_SAMPLE_DATA) {
  # If all the sample files don't already exist for this sample rate, create them
  if (! all(file.exists(unlist(enSampleFile)))) {
    if (! dir.exists(enUsSampleDir)) {
      dir.create(enUsSampleDir)
    }
    
    # Read all files, generate a random sample and save it to another file for later use
    set.seed(1234)
    for(i in 1:length(enFile)) {
      tempData <- readLines(enFile[i])
      # Only keep a random subset before creating the training, validation and test sets
      # Done because files are large and we want the validation and test sets to have held out samples
      tempData <- sample(tempData, as.integer(FILE_SAMPLE_RATE * length(tempData)))
      
      # Training set
      inTraining <- seq_along(tempData)
      inTraining <- sample(inTraining, as.integer(TRAINING_SAMPLE_RATE * length(inTraining)))
      trainSet <- tempData[inTraining]
      
      # Validation and test sets
      validAndTestSet <- tempData[-inTraining]
      inTest <- seq_along(validAndTestSet)
      inTest <- sample(inTest, as.integer(0.5 * length(inTest)))  # 0.5 b/c validation and test sets have same size
      testSet <- validAndTestSet[inTest]
      validSet <- validAndTestSet[-inTest]
      
      # Save sampled datasets in files
      writeLines(trainSet, enSampleFile[[i]][1])
      writeLines(validSet, enSampleFile[[i]][2])
      writeLines(testSet, enSampleFile[[i]][3])
    }
    # Clean up memory (not very useful in general apparently)
    rm(tempData)
    rm(trainSet)
    rm(validSet)
    rm(testSet)
    gc()  # garbage collector
  }
  # Sample files here, nothing to do
}
```


## Load training set into a Corpus
```{r cache = TRUE, echo = FALSE, results = FALSE}
# Only load Corpus if the tokenized sentences don't exist in a RDS file for this version and sample rate
if(!file.exists(rdsFileSentences)) {
  if (USE_SAMPLE_DATA) {
    corpusFiles <- character()
    
    # For each document, only load the training set
    for(i in 1:length(enSampleFile)) {
      corpusFiles <- c(corpusFiles, enSampleFile[[i]][1]) # Training set always first
    }
  } else {
    corpusFiles <- enFile
  }
  # Load documents as a Quanteda Corpus
  enRawCorpus <- corpus(textfile(corpusFiles, encoding = "UTF-8"))
  #summary(enRawCorpus)
}
```

## Tokenizing functions
The corpus should be tokenized as sentences before creating the n-grams tokens. This makes sure that no n-gram will be created with the end of sentence and the beginning of the next.
```{r results = FALSE}
# Get vector of swear words to remove
# File downloaded from http://www.bannedwordlist.com/lists/swearWords.csv
getSwearWords <- function() {
  swearWords <- readLines(swearWordsFile, skipNul = TRUE, warn = FALSE) # warning because of missing "end of line" character
  swearWords <- unlist(strsplit(swearWords, ",", fixed = TRUE))
  swearWords
}

# Sentence tokenization function
tokenizeSentences <- function(x) {
  # TODO : remove twitter characters here manually
  
  tokens <- tokenize(x, what = "sentence", removeNumbers = TRUE, removePunct = TRUE, removeSymbols = TRUE
                     , removeSeparators = TRUE, removeTwitter = FALSE, removeHyphen = TRUE, removeURL = TRUE)
  
  # Add start and end of sentences symbols
  # #bos# = beginning of sentence
  # #eos# = end of sentence (like EOF symbol for "end of file")
  
  # Symbols added because of the Ngrams chapter in the book Speech and Language Processing
  # by Daniel Jurafsky and James Martin
  # Not sure if these symbols will be used in the prediction model
  # WARNING : these symbols cannot use punctuation that would be removed in the n-grams tokenization
  
  # ************************************************************************************
  # TODO : see if start and end of sentences useful for final model.
  # => If used : 
  #       - set removeTwitter to FALSE in the tokenizeNgrams function
  #       - Remove Twitter content with regexp manually after the ngram tokenization
  # ************************************************************************************
  #unlist(lapply(tokens, function(y) paste('#bos#', toLower(y), '#eos#')))
  unlist(lapply(tokens, toLower))
}

# N-grams tokenization function
# Default is unigram if n is not set
# removeTwitter needs to be FALSE in case start and end of sentence markers used (# character)
tokenizeNgrams <- function(x, n = 1L) {
  tokenize(x, what = "word", removeNumbers = TRUE, removePunct = TRUE, removeSymbols = TRUE
           , removeSeparators = TRUE, removeTwitter = TRUE, removeHyphen = TRUE, removeURL = TRUE
           , ngrams = n, simplify = TRUE)
}
```

## Tokenizing
### Sentences
```{r cache = TRUE}
# If a previous RDS file exists for this version and sample rate, load it
# Else compute the tokens and save it to a RDS file
if(file.exists(rdsFileSentences)) {
  sentenceTokens <- readRDS(rdsFileSentences)
} else {
  # Always tokenize sentences first
  #sentenceTokens <- tokenizeSentences(enRawCorpus)
  system.time(sentenceTokens <- tokenizeSentences(enRawCorpus))
  saveRDS(sentenceTokens, rdsFileSentences)
}
```

### 1-grams
Features are ignored in the dfm step to avoid creating wrong ngrams when n > 1 (because of a word being removed from a ngram).

See the "Details" section of dfm's help page.
```{r cache = TRUE}
# If a previous RDS file exists for this version and sample rate, load it
# Else compute the tokens and save it to a RDS file
if(file.exists(rdsFileDtDfm1)) {
  dtDfm1 <- readRDS(rdsFileDtDfm1)
} else {
  #ngram1 <- tokenizeNgrams(sentenceTokens, 1)
  system.time(ngram1 <- tokenizeNgrams(sentenceTokens, 1))
  #dfm1 <- dfm(ngram1, ignoredFeatures=getSwearWords())
  system.time(dfm1 <- dfm(ngram1, ignoredFeatures=getSwearWords()))
  # 1.1 Gb for 15%
  
  # Convert DFM to data table format (faster and uses less memory)
  dtDfm1<- data.table(ngram = features(dfm1), count = colSums(dfm1), key = "ngram")
  saveRDS(dtDfm1, rdsFileDtDfm1)
}
```

### 2-grams
```{r cache = TRUE}
# If a previous RDS file exists for this version and sample rate, load it
# Else compute the tokens and save it to a RDS file
if(file.exists(rdsFileDtDfm2)) {
  dtDfm2 <- readRDS(rdsFileDtDfm2)
} else {
  #ngram2 <- tokenizeNgrams(sentenceTokens, 2)
  system.time(ngram2 <- tokenizeNgrams(sentenceTokens, 2))
  #dfm2 <- dfm(ngram2, ignoredFeatures=getSwearWords())
  system.time(dfm2 <- dfm(ngram2, ignoredFeatures=getSwearWords()))
  
  # Convert DFM to data table format (faster and uses less memory)
  dtDfm2<- data.table(ngram = features(dfm2), count = colSums(dfm2), key = "ngram")
  saveRDS(dtDfm2, rdsFileDtDfm2)
}
```

### 3-grams
```{r cache = TRUE}
# If a previous RDS file exists for this version and sample rate, load it
# Else compute the tokens and save it to a RDS file
if(file.exists(rdsFileDtDfm3)) {
  dtDfm3 <- readRDS(rdsFileDtDfm3)
} else {
  #ngram3 <- tokenizeNgrams(sentenceTokens, 3)
  system.time(ngram3 <- tokenizeNgrams(sentenceTokens, 3))
  #dfm3 <- dfm(ngram3, ignoredFeatures=getSwearWords())
  system.time(dfm3 <- dfm(ngram3, ignoredFeatures=getSwearWords()))
  # 1.5 Gb for 15%
  
  # Convert DFM to data table format (faster and uses less memory)
  dtDfm3<- data.table(ngram = features(dfm3), count = colSums(dfm3), key = "ngram")
  saveRDS(dtDfm3, rdsFileDtDfm3)
}
```

### 4-grams
```{r cache = TRUE}
# If a previous RDS file exists for this version and sample rate, load it
# Else compute the tokens and save it to a RDS file
if(file.exists(rdsFileDtDfm4)) {
  dtDfm4 <- readRDS(rdsFileDtDfm4)
} else {
  #ngram4 <- tokenizeNgrams(sentenceTokens, 4)
  system.time(ngram4 <- tokenizeNgrams(sentenceTokens, 4))
  #dfm4 <- dfm(ngram4, ignoredFeatures=getSwearWords())
  system.time(dfm4 <- dfm(ngram4, ignoredFeatures=getSwearWords()))
  # 1.7 Gb for 15%
  
  # Convert DFM to data table format (faster and uses less memory)
  dtDfm4<- data.table(ngram = features(dfm4), count = colSums(dfm4), key = "ngram")
  saveRDS(dtDfm4, rdsFileDtDfm4)
}
```

## TODO : distro of the DFM to see how to set MIN_NGRAM_COUNT and remove ngrams with low frequency

## Analysing the most common n-grams
### Keep only the `r I(TOP_NGRAM_LIMIT)` most common n-grams for each document-term matrix
```{r}
dtFreq1Top <- head(dtDfm1[order(count, decreasing = TRUE)], TOP_NGRAM_LIMIT)
dtFreq2Top <- head(dtDfm2[order(count, decreasing = TRUE)], TOP_NGRAM_LIMIT)
dtFreq3Top <- head(dtDfm3[order(count, decreasing = TRUE)], TOP_NGRAM_LIMIT)
dtFreq4Top <- head(dtDfm4[order(count, decreasing = TRUE)], TOP_NGRAM_LIMIT)
```

### Plot the most common n-grams
```{r}
# Most common 1-grams plot
g1 <- ggplot(dtFreq1Top, aes(x = reorder(ngram, -count), y = count))
g1 <- g1 + geom_bar(stat="identity", fill = "deepskyblue4")
g1 <- g1 + ggtitle("Top unigrams sorted by frequency")
g1 <- g1 + xlab("Unigram") + ylab("Count")
g1 <- g1 + theme(axis.text.x=element_text(angle=45, hjust=1))
g1

# Most common 2-grams plot
g2 <- ggplot(dtFreq2Top, aes(x = reorder(ngram, -count), y = count))
g2 <- g2 + geom_bar(stat="identity", fill = "deepskyblue4")
g2 <- g2 + ggtitle("Top bigrams sorted by frequency")
g2 <- g2 + xlab("Bigram") + ylab("Count")
g2 <- g2 + theme(axis.text.x=element_text(angle=45, hjust=1))
g2

# Most common 3-grams plot
g3 <- ggplot(dtFreq3Top, aes(x = reorder(ngram, -count), y = count))
g3 <- g3 + geom_bar(stat="identity", fill = "deepskyblue4")
g3 <- g3 + ggtitle("Top trigrams sorted by frequency")
g3 <- g3 + xlab("Trigram") + ylab("Count")
g3 <- g3 + theme(axis.text.x=element_text(angle=45, hjust=1))
g3

# Most common 4-grams plot
g4 <- ggplot(dtFreq4Top, aes(x = reorder(ngram, -count), y = count))
g4 <- g4 + geom_bar(stat="identity", fill = "deepskyblue4")
g4 <- g4 + ggtitle("Top fourgrams sorted by frequency")
g4 <- g4 + xlab("Fourgram") + ylab("Count")
g4 <- g4 + theme(axis.text.x=element_text(angle=45, hjust=1))
g4
```

## Prediction prototype
```{r}


# TODO : remove manual prediction for quiz

regex <- "thank_you_so"
hits <- system.time(dtDfm4[ngram %like% paste("^", regex, "_", sep = ""), ][order(count, decreasing = TRUE)])



# TODO : backoff / smoothing (=??) / score calculation
```



## TODO : smoothing (= ???) and backoff (+ see Markov link to data table format)
